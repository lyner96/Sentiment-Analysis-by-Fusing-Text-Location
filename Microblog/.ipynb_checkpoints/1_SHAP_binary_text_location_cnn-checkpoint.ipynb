{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "existing-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lyner/Documents/SA_Fusing_Text_Location/venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "seed_value= 7\n",
    "\n",
    "# Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tensorflow.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "tensorflow.compat.v1.disable_v2_behavior()\n",
    "import pandas as pd\n",
    "from tensorflow.compat.v1.keras import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "from tensorflow.compat.v1.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.compat.v1.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.compat.v1.keras.models import Model\n",
    "from tensorflow.compat.v1.keras.initializers import Constant\n",
    "from tensorflow.compat.v1.keras.layers import Layer, Convolution1D, Embedding, Dense, Flatten, MaxPooling1D, Input, Bidirectional, LSTM, GlobalAveragePooling1D, LayerNormalization, Dropout\n",
    "from tensorflow.compat.v1.keras.callbacks import EarlyStopping\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mineral-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(task, input_concat, model_type, i):\n",
    "    \n",
    "    '''\n",
    "    task: string\n",
    "        binary: Dataset with sentiment of postive and negative.\n",
    "        multiclass: Dataset with sentiment of positive, neutral, and negative.\n",
    "        \n",
    "    input_concat: boolean\n",
    "        True: The input is concatenated.\n",
    "        False: The input is seperated.\n",
    "    \n",
    "    model_type: string\n",
    "        CNN\n",
    "        BiLSTM\n",
    "        Transformer\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    ### Dataset selection\n",
    "    \n",
    "    if task == 'multiclass':\n",
    "        # dataset for multiclass\n",
    "        df = pd.read_csv('data/geo_microblog.csv')\n",
    "    else:\n",
    "        # dataset for binary\n",
    "        df = pd.read_csv('data/geo_microblog.csv')\n",
    "        df = df[df.sentiment != 1]\n",
    "        df.sentiment.replace(2, 1, inplace=True) # neutral\n",
    "\n",
    "    ### Text processing\n",
    "    \n",
    "    # prepare tokenizer\n",
    "    \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(df['text'])\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    \n",
    "    # integer encode the documents\n",
    "    \n",
    "    encoded_docs = t.texts_to_sequences(df['text'])\n",
    "    txtlen = 30\n",
    "    loclen = 0\n",
    "    \n",
    "    # pad documents to a max length \n",
    "    \n",
    "    padded_docs = pad_sequences(encoded_docs, txtlen, padding='post')\n",
    "\n",
    "    ### Location processing\n",
    "    \n",
    "    # location = df['geonames'].apply(lambda x: pd.Series([i for i in x.split(',') if i not in [\"\"]]).value_counts())\n",
    "    # location = location.reindex(sorted(location.columns), axis=1).fillna(0)\n",
    "    # location = location.values\n",
    "    # np.save('microblog_multiclass_location.npy', location)\n",
    "    if task == 'binary':\n",
    "        location = np.load('variable/microblog_binary_location.npy')\n",
    "    else:\n",
    "        location = np.load('variable/microblog_multiclass_location.npy')\n",
    "    loclen = location.shape[1]\n",
    "        \n",
    "    ### Prepare train and test set\n",
    "        \n",
    "    # merge txt and loc\n",
    "    \n",
    "    merge = np.concatenate((padded_docs, location),axis=1)\n",
    "    \n",
    "    # divide dataset to train and test set\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(merge, df['sentiment'], test_size=0.3, random_state=100)\n",
    "        \n",
    "    if input_concat == False:\n",
    "        # split train set to text and location\n",
    "        x_train1 = x_train[:,:txtlen]\n",
    "        x_train2 = x_train[:,-loclen:]\n",
    "        \n",
    "        # # split test set to text and location\n",
    "        x_test1 = x_test[:,:txtlen]\n",
    "        x_test2 = x_test[:,-loclen:]\n",
    "        \n",
    "    ### Pretrained word embedding\n",
    "    \n",
    "    # load the whole embedding into memory\n",
    "    \n",
    "    #embeddings_index = dict()\n",
    "    #f = open('../glove.twitter.27B.200d.txt')\n",
    "    #for line in f:\n",
    "    #\tvalues = line.split()\n",
    "    #\tword = values[0]\n",
    "    #\tcoefs = asarray(values[1:], dtype='float32')\n",
    "    #\tembeddings_index[word] = coefs\n",
    "    #f.close()\n",
    "    #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    \n",
    "    vector_dimension = 200\n",
    "    #embedding_matrix = zeros((vocab_size, vector_dimension))\n",
    "    #for word, i in t.word_index.items():\n",
    "    #\tembedding_vector = embeddings_index.get(word)\n",
    "    #\tif embedding_vector is not None:\n",
    "    #\t\tembedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    if task == 'binary':\n",
    "        embedding_matrix = np.load('variable/microblog_binary_embedding_matrix.npy')\n",
    "    else:\n",
    "        embedding_matrix = np.load('variable/microblog_multiclass_embedding_matrix.npy')\n",
    "    \n",
    "    ### Deep Learning model\n",
    "    \n",
    "    if input_concat == True:\n",
    "        input_dimension = txtlen+loclen\n",
    "        inputs = Input(shape=(input_dimension,))\n",
    "        embedding_layer = Embedding(vocab_size, vector_dimension, embeddings_initializer=Constant(embedding_matrix), input_length=input_dimension)(inputs)\n",
    "    else:\n",
    "        inputText = Input(shape=(txtlen,))\n",
    "        x = Embedding(vocab_size, vector_dimension, embeddings_initializer=Constant(embedding_matrix), input_length=txtlen)(inputText)\n",
    "        inputLocation = Input(shape=(loclen,))\n",
    "        y = Embedding(vocab_size, vector_dimension, embeddings_initializer=RandomNormal(), input_length=loclen)(inputLocation)\n",
    "        embedding_layer = concatenate([x, y], axis=1)\n",
    "    \n",
    "    if model_type == \"CNN\":\n",
    "        # CNN\n",
    "        convolution_first = Convolution1D(filters=100, kernel_size=5, activation='relu')(embedding_layer)\n",
    "        convolution_second = Convolution1D(filters=100, kernel_size=4, activation='relu')(convolution_first)\n",
    "        convolution_third = Convolution1D(filters=100, kernel_size=3, activation='relu')(convolution_second)\n",
    "        pooling_max = MaxPooling1D(pool_size=2)(convolution_third)\n",
    "        flatten_layer = Flatten()(pooling_max)\n",
    "        dense = Dense(20, activation=\"relu\")(flatten_layer)\n",
    "        if task == 'binary':\n",
    "            outputs = Dense(units=1, activation='sigmoid')(dense)\n",
    "        else:\n",
    "            outputs = Dense(units=3, activation='softmax')(dense)\n",
    "    \n",
    "    if model_type == \"BiLSTM\":\n",
    "        ### BiLSTM\n",
    "        lstm_first = Bidirectional(LSTM(units=100))(embedding_layer)\n",
    "        dense = Dense(20, activation=\"relu\")(lstm_first)\n",
    "        if task == 'binary':\n",
    "            outputs = Dense(1, activation='sigmoid')(dense)\n",
    "        else:\n",
    "            outputs = Dense(3, activation='softmax')(dense)\n",
    "            \n",
    "    if model_type == \"Transformer\":\n",
    "        ### Transformer\n",
    "        num_heads = 2  # Number of attention heads\n",
    "        ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "        \n",
    "        if input_concat == True:\n",
    "            embedding_layer_weighted = TokenAndPositionEmbedding(input_dimension, vocab_size, vector_dimension, Constant(embedding_matrix))\n",
    "            x = embedding_layer_weighted(inputs)\n",
    "        else:\n",
    "            embedding_layer_weighted = TokenAndPositionEmbedding(txtlen, vocab_size, vector_dimension, Constant(embedding_matrix))\n",
    "            x = embedding_layer_weighted(inputText)\n",
    "            embedding_layer = TokenAndPositionEmbedding(loclen, vocab_size, vector_dimension, RandomNormal())\n",
    "            y = embedding_layer(inputLocation)\n",
    "            x = concatenate([x, y], axis=1)\n",
    "        \n",
    "        transformer_block = TransformerBlock(vector_dimension, num_heads, ff_dim)\n",
    "        x = transformer_block(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dense(20, activation=\"relu\")(x)\n",
    "        if task == 'binary':\n",
    "            outputs = Dense(1, activation='sigmoid')(x)\n",
    "        else:\n",
    "            outputs = Dense(3, activation='softmax')(x)\n",
    "                \n",
    "    \n",
    "    # build model\n",
    "    \n",
    "    if input_concat == True: \n",
    "        model = Model(inputs, outputs)\n",
    "    else:\n",
    "        model = Model(inputs=[inputText, inputLocation], outputs=outputs)\n",
    "    \n",
    "    # compile model\n",
    "    \n",
    "    if task == 'binary':\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])    \n",
    "\n",
    "    # early stopping\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(patience=3) #patience: epochs the model can go without improving before we stop training\n",
    "\n",
    "        \n",
    "    if input_concat == True:\n",
    "                    \n",
    "        model.load_weights(f\"saved/{task}/2_loc/concat/{model_type}/{task}_location_True_concat_{model_type}_{i}.h5\")\n",
    "         \n",
    "        # evaluate model\n",
    "        \n",
    "        loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        model.load_weights(f\"saved/{task}/2_loc/both/{model_type}/{task}_location_True_both_{model_type}_{i}.h5\")\n",
    "        \n",
    "        # evaluate model\n",
    "        loss, accuracy = model.evaluate([x_test1, x_test2], y_test, verbose=1)\n",
    "    \n",
    "    return model, loss, accuracy, x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "million-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run \n",
    "\n",
    "task = 'binary'\n",
    "input_concat = True\n",
    "model_type = \"CNN\"\n",
    "i = 0\n",
    "\n",
    "model, loss, accuracy, x_train, x_test= run(task, input_concat, model_type, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-practice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518129"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lyner/Documents/SA_Fusing_Text_Location/venv/lib/python3.8/site-packages/shap/explainers/tf_utils.py:28: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_sample = shap.sample(x_train, 100)\n",
    "X_test_sample = shap.sample(x_test, 20)\n",
    "explainer = shap.DeepExplainer(model, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "shap.summary_plot(shap_values, X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-university",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-links",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-burst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-affairs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-metadata",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-obligation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-stupid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
