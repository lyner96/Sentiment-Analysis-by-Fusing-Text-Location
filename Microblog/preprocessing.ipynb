{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "1. preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cMSheJiynY1"
      },
      "source": [
        "Load data before pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvYtJZPSynY4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv('full_text.txt', sep='\\t', encoding='latin-1', header=None, error_bad_lines=False)\n",
        "\n",
        "# drop unused columns\n",
        "df.drop([0, 1, 2], axis=1, inplace=True)\n",
        "\n",
        "# rename colums\n",
        "df.columns = ['lat', 'lon', 'text']\n",
        "#data.rename(columns={\"a_sentiment\": \"sentiment\"}, inplace=True)\n",
        "\n",
        "# drop duplicates rows\n",
        "df.drop_duplicates(subset=['lat', 'lon'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "data = df.copy()\n",
        "\n",
        "# select 10k samples\n",
        "#data = df.sample(n = 10000, random_state = 100, axis=0, replace=False).reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig5c2uIDynZG"
      },
      "source": [
        "Process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2cVb6M9ynZM"
      },
      "source": [
        "# lower casing\n",
        "#data['text'] = data['text'].str.lower()\n",
        "# removes emoticons\n",
        "#data['text'] = data['text'].astype(str).apply(lambda x: \" \".join(re.sub(r':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', x) for x in x.split()))\n",
        "\n",
        "# remove unicode strings\n",
        "data['text'] = data['text'].str.replace(r\"(\\\\u[0-9A-Fa-f]+)\",\"\")\n",
        "data['text'] = data['text'].astype(str).str.replace(r\"[^\\x00-\\x7f]\",\"\")\n",
        "\n",
        "# convert any url to URL\n",
        "data['text'] = data['text'].str.replace(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\")\n",
        "\n",
        "# remove @Username, RT\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"@[^\\s]+\", \"\")\n",
        "data[\"text\"] = data[\"text\"].str.replace(r\"RT\", \"\")\n",
        "\n",
        "# remove punctuations, numbers, special characters except characters and hashtags with space\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# remove duplicate character\n",
        "data[\"text\"] = data[\"text\"].str.replace(r\"([a-z])(\\1{3,})\", \"\")\n",
        "\n",
        "# remove additional white spaces\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"[\\s]+\", \" \")\n",
        "data[\"text\"] = data[\"text\"].str.replace(\"[\\n]+\", \" \")\n",
        "data[\"text\"] = data[\"text\"].str.replace(r\"^[\\s]\", \"\")\n",
        "        \n",
        "# keep text having length more than 5\n",
        "data = data[data['text'].str.len() > 5]\n",
        "                          \n",
        "# remove empty row\n",
        "data.drop_duplicates(subset='text', keep=False, inplace=True)\n",
        "\n",
        "# reset index\n",
        "data.reset_index(drop = True, inplace=True)\n",
        "\n",
        "# save dataframe to local\n",
        "data.to_csv('dataset/cleaned.csv', sep=',', index=False)\n",
        "#data = pd.read_csv('dataset/geo.csv')\n",
        "\n",
        "'''\n",
        "Check tweet length\n",
        "'''\n",
        "\n",
        "# create empty list\n",
        "tweet_length = []\n",
        "\n",
        "# loop through dataset\n",
        "for i,t in enumerate(df.text):\n",
        "    # every legth of tweets\n",
        "    tweet_length.append(len(str(t)))\n",
        "\n",
        "# drop tweet that less than 15 words\n",
        "for i,t in enumerate(df.text):    \n",
        "    if len(str(df.text[i])) < 15:\n",
        "        df.drop(i, inplace=True)\n",
        "\n",
        "# average length of tweet\n",
        "avg_tweet_length = sum(tweet_length)/len(df.text)\n",
        "\n",
        "df = df[df[len(str(df.text)) > 15]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IedTV9QynZV"
      },
      "source": [
        "Calculate sentiment polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4JmXvfdynZZ"
      },
      "source": [
        "'''\n",
        "vader\n",
        "'''\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "def calculate_vader(sentence):\n",
        "    score = analyser.polarity_scores(str(sentence))\n",
        "    return score['compound']\n",
        "\n",
        "data['vader_polarity'] = data['text'].apply(calculate_vader)\n",
        "\n",
        "'''\n",
        "textblob\n",
        "'''\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def calculate_blob(sentence):\n",
        "   blob = TextBlob(str(sentence))\n",
        "   score = blob.sentiment.polarity\n",
        "   return score\n",
        "\n",
        "data['blob_polarity'] = data['text'].apply(calculate_blob)\n",
        "\n",
        "'''\n",
        "senticnet\n",
        "'''\n",
        "\n",
        "from senticnet.senticnet import SenticNet\n",
        "\n",
        "sn = SenticNet()\n",
        "\n",
        "def calculate_sentic(sentence):\n",
        "    sentic = sn.polarity_intense('i love you')\n",
        "    return sentic\n",
        "\n",
        "from sentic import SenticPhrase\n",
        "sp = SenticPhrase('i love u')\n",
        "sp.get_polarity()\n",
        "\n",
        "'''\n",
        "watsons\n",
        "'''\n",
        "\n",
        "from ibm_watson import NaturalLanguageUnderstandingV1\n",
        "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions\n",
        "\n",
        "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
        "    version='2019-07-12',\n",
        "    iam_apikey='',\n",
        "    url='https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2019-07-12'\n",
        ")\n",
        "\n",
        "def Sentiment_score(input_text): \n",
        "    # Input text can be sentence, paragraph or document\n",
        "    response = natural_language_understanding.analyze (\n",
        "    text = str(input_text),\n",
        "    features = Features(sentiment=SentimentOptions())).get_result()\n",
        "    # From the response extract score which is between -1 to 1\n",
        "    res = response.get('sentiment').get('document').get('score')\n",
        "    return res\n",
        "\n",
        "df['watson_score'] = ''\n",
        "for i,t in enumerate(df.text):\n",
        "    response = natural_language_understanding.analyze(text = str(t),features = Features(sentiment=SentimentOptions())).get_result()\n",
        "    df['watson_score'].iloc[i] = response.get('sentiment').get('document').get('score')\n",
        "\n",
        "df['watson_score'] = df['text'].apply(Sentiment_score)\n",
        "\n",
        "def w_s(sen):\n",
        "    response = natural_language_understanding.analyze(text = str(t),features = Features(sentiment=SentimentOptions())).get_result()\n",
        "    score = response.get('sentiment').get('document').get('score')\n",
        "    return score\n",
        "\n",
        "\n",
        "'''\n",
        "polyglot\n",
        "'''\n",
        "\n",
        "from polyglot.text import Text\n",
        "\n",
        "def calculate_polyglot(sentence):\n",
        "    text = Text(str(sentence))\n",
        "    score = text.polarity\n",
        "    return score\n",
        "\n",
        "def qc(sentence):\n",
        "    text = Text(str(sentence))\n",
        "    score = text.polarity\n",
        "    lang = text.language.code\n",
        "    return score, lang\n",
        "\n",
        "df['polyglot_score'] = df['text'].apply(calculate_polyglot)\n",
        "\n",
        "\n",
        "df['polyglot_score'] = ''\n",
        "\n",
        "for i,t in zip(df.index.values, df.text):\n",
        "    text = Text(t)\n",
        "    df['polyglot_score'].loc[i] = text.polarity\n",
        "\n",
        "# drop row of score 0\n",
        "for i,x in tqdm(enumerate(data.score)):\n",
        "    if data.loc[i, 'blob_score'] == 0:\n",
        "        data.drop(i, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFilCRVMynZg"
      },
      "source": [
        "Label sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvOrNvPzynZi"
      },
      "source": [
        "def classify_asentiment(polarity): # label not balanced\n",
        "    if(polarity >= 0.5): \n",
        "        return int(2) #postive\n",
        "    elif(polarity <= -0.5): \n",
        "        return int(0) #negative\n",
        "    else:\n",
        "        return int(1) #neutral\n",
        "\n",
        "def classify_sentiment(polarity):\n",
        "    if(polarity > 0): #postive\n",
        "        return int(2)\n",
        "    elif(polarity < 0): #negative\n",
        "        return int(0)\n",
        "    else: #neutral\n",
        "        return int(1)\n",
        "    \n",
        "# normalize sentiment score and append to dataframe\n",
        "data['sentiment'] = data['vader_polarity'].apply(classify_sentiment)\n",
        "df['sentiment'] = df['vader_polarity'].apply(classify_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdFyRamfynZp"
      },
      "source": [
        "Location processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj7t1AXFynZr"
      },
      "source": [
        "\n",
        "'''\n",
        "Nearby entities - Geonames\n",
        "'''\n",
        "\n",
        "from tqdm import tqdm\n",
        "import requests, json\n",
        "\n",
        "# create new column in dataframe\n",
        "df['geonames'] = ''\n",
        "\n",
        "# loop through index, lat, lon from dataframe\n",
        "for i,x,y in tqdm(zip(df.index.values[10500:10750], df.lat[10500:10750], df.lon[10500:10750])):\n",
        "    # convert new column to list\n",
        "    df['geonames'].iloc[i] = []\n",
        "    # request data \n",
        "    r = requests.get(\"http://api.geonames.org/findNearbyJSON?lat=\"+str(x)+\"&lng=\"+str(y)+\"&radius=0.3&username=\"+'') # radius is in kilo meters (km)\n",
        "    # get text data\n",
        "    #r.text\n",
        "    # load to dictionary\n",
        "    data = json.loads(r.text)\n",
        "    # calculate length of list\n",
        "    n = len(data['geonames'])\n",
        "    # loop through list\n",
        "    for c in range(0, n):\n",
        "        # add data to new column\n",
        "        df['geonames'].iloc[i].append(data['geonames'][c]['fcodeName'])\n",
        "\n",
        "for i,x,y in tqdm(zip(df.index.values[10750:11000], df.lat[10750:11000], df.lon[10750:11000])):\n",
        "    # convert new column to list\n",
        "    df['geonames'].iloc[i] = []\n",
        "    # request data \n",
        "    r = requests.get(\"http://api.geonames.org/findNearbyJSON?lat=\"+str(x)+\"&lng=\"+str(y)+\"&radius=0.3&username=\"+'') # radius is in kilo meters (km)\n",
        "    # get text data\n",
        "    #r.text\n",
        "    # load to dictionary\n",
        "    data = json.loads(r.text)\n",
        "    # calculate length of list\n",
        "    n = len(data['geonames'])\n",
        "    # loop through list\n",
        "    for c in range(0, n):\n",
        "        # add data to new column\n",
        "        df['geonames'].iloc[i].append(data['geonames'][c]['fcodeName'])\n",
        "        \n",
        "df.to_csv('neu.csv', sep=',', index=False)\n",
        "\n",
        "'''\n",
        "Nearby entities - Google \n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import requests, json\n",
        "\n",
        "API_KEY = ''\n",
        "\n",
        "# create new column in dataframe\n",
        "df['google'] = ''\n",
        "\n",
        "# loop through index, lat, lon from dataframe\n",
        "for i,x,y in tqdm(zip(df.index.values, df.lat, df.lon)):\n",
        "    # convert new column to list\n",
        "    df['google'].iloc[i] = []\n",
        "    # request data \n",
        "    r = requests.get(\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=\"+str(x)+\",\"+str(y)+\"&radius=300&key=\"+API_KEY) # radis is in meters (m)\n",
        "    # get text data\n",
        "    r.text\n",
        "    # load to dictionary\n",
        "    data = json.loads(r.text)\n",
        "    # calculate length of list\n",
        "    n = len(data['geonames'])\n",
        "    # loop through list\n",
        "    for c in range(0, n):\n",
        "        # add data to new column\n",
        "        df['geonames'].iloc[i].append(data['results'][c]['types'][0]) # get the first type\n",
        "        \n",
        "# clean location\n",
        "df['geonames'] = df['geonames'].astype(str).apply(lambda x: \" \".join(re.sub(\"\\[|'|]\", '', x) for x in x.split()))\n",
        "df['geonames'] = df['geonames'].astype(str).apply(lambda x: \" \".join(re.sub(\"^,\", '', x) for x in x.split()))\n",
        "df['geonames'] = df['geonames'].astype(str).apply(lambda x: \" \".join(re.sub('[\\s]+', '', x) for x in x.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC9k9gFaynZ0"
      },
      "source": [
        "Location processing with Ego Network 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiS41LR-ynZ1"
      },
      "source": [
        "'''\n",
        "network density = num of actual ties / num of node * (num of node - 1) / 2\n",
        "closeness centrality = 1 / farness - sum of distance to other nodes\n",
        "degree centrality = how many ties touch a node\n",
        "betweeness centralitity = how many times to pass by a node in between\n",
        "'''\n",
        "\n",
        "def ego_network_density(categories):\n",
        "    categories_count = len(categories.split(','))\n",
        "    density = categories_count / (categories_count + 1 * categories_count / 2) # + 1 is tweet with categories\n",
        "    return density\n",
        "\n",
        "def ego_network_closeness(categories):\n",
        "    categories_count = len(categories.split(','))\n",
        "    closeness = 1 / categories_count\n",
        "    return closeness\n",
        "\n",
        "def ego_network_degree(categories):\n",
        "    categories_count = len(categories.split(','))\n",
        "    degree = categories_count\n",
        "    return degree\n",
        "\n",
        "df['ego_density'] = df['geonames'].apply(ego_network_density)\n",
        "df['ego_closeness'] = df['geonames'].apply(ego_network_closeness)\n",
        "df['ego_degree'] = df['geonames'].apply(ego_network_degree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY1fudDWz-QJ"
      },
      "source": [
        "Categories count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmiJn-tVzm6_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('microblog.csv')\n",
        "\n",
        "loc = df.geonames.str.get_dummies(',')\n",
        "place = list(loc)\n",
        "loc2 = pd.DataFrame(index = place, columns = ['positive', 'neutral', 'negative']).fillna(0)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    categories = df.geonames[i].split(',')\n",
        "    \n",
        "    if df.sentiment[i] == 0:\n",
        "        for sub in categories:\n",
        "            loc2['negative'][sub] += 1\n",
        "    elif df.sentiment[i] == 1:\n",
        "        for sub in categories:\n",
        "            loc2['neutral'][sub] += 1\n",
        "    elif df.sentiment[i] == 2:\n",
        "        for sub in categories:\n",
        "            loc2['positive'][sub] += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
